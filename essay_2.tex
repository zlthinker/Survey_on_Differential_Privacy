%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\DeclareRobustCommand{\eg}{e.g.\@\xspace}
\DeclareRobustCommand{\ie}{i.e.\@\xspace}
\DeclareRobustCommand{\etal}{et al.\@\xspace}

\makeatletter
\DeclareRobustCommand{\etc}{%
    \@ifnextchar{.}%
        {etc}%
        {etc.\@\xspace}%
}
\makeatother

%\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images

\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

\begin{document}
%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Differential Privacy}\\ % Title
A Survery} % Subtitle

\author{\textsc{20398702 HU, Jiajun \\ 20304086 ZHOU, Lei} % Author
\\{\textit{Department of Computer Science \& Engineering \\ The Hong Kong University of Science and Technology}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------


\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else

\begin{abstract}
Differential privacy imposes a strong privacy guarantee on the database that the romoval or addition of a single record does not significantly affect the outcome of any analysis made on the database. In this survey, we review the definition of differential privacy and revisit a number of approaches proposed recently that make significant contributions to the advance of differential privacy.
\end{abstract}

%\hspace*{3,6mm}\textit{Keywords:} differential privacy % Keywords

\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section{Introduction}
With the fast development of web techologies, the number of interent users are growing exponentially. In order to provide better services, many service providers, such as government, company, and research institutes, collect personal information and analyze them. Social networks such as Facebook and LinkedIn use friendship to recommend new friends to you. Youtube \& Amazon use viewing/buying records also for recommendations. Emails in Gmail are used for targeted Ads. We are enjoying the big convenience and effecience brought by these technologies, however, we are still under the risk of personal information leakage. Trouble will be made if attackers exploit the leaked information and make use of them.

To address this issue, a common practice is to anonymize certain data in the database. On could remove or hash the identifiable information, such as name and identity number, in the database. HIPAA \footnote{https://en.wikipedia.org/wiki/Health\textunderscore Insurance\textunderscore Portability \textunderscore and\textunderscore Accountability\textunderscore Act}  (Health Insurance Portability and Accountability Act) has promoted this approach and list 18 categories of identifiable information which can be anonymized. However, anonymizing data is insufficient especially when the attacker has already known some background knowledge about the individuals in the database. To combat this issue, some advanced technologies, such as k-anonymity\cite{sweeney2002k}, l-diversity\cite{machanavajjhala2007diversity} and t-closeness\cite{li2007t}, have been proposed. But it turns out even these techologies cannot prevent background attacks especially when the attacker already know something about the contents in the database\cite{ji2014differential}.

In this survey, we review differential privacy\cite{dwork2008differential, dwork2014algorithmic} . Differential privacy is a powerful approach to privatize the released information from database. Even in the WWDC 2016\footnote{https://developer.apple.com/videos/wwdc2016}, Apple announced a series of new security and privacy features, including one feature that is differential privacy, with the aim to improve the privacy of their data collection practices\footnote{https://www.wired.com/2016/06/apples-differential-privacy-collecting-data/}. Roughly speaking, differential privacy ensures that the removal or insertion of a single record does not significantly affect the outcome of any analysis made on the database, thus making it possible to prevent attackers from gussing the real contents of the database. It follows a rigorous mathematical deduction to prove it can reduce the risk of privacy breach while remaining the utility of the data.

In the rest of the survey, we provide the definition of differential privacy and three mechanisms, Laplace Mechanism, Exponential Mechanism and BLR Mechanism (Section 2). Then, we discuss a special case of differential privacy, local differential privacy (LDP), and list four typical LDP solutions (Section 3). Finally, we conclude the survey in the last section.


%------------------------------------------------

\section{Differentail Privacy}

Over the past ten years, differentail privacy\cite{dwork2008differential, dwork2014algorithmic} has emerged to become one of the most powerful approaches to ensure data pricacy. Roughly speaking, differential privacy ensures that the removal or insertion of a single record does not significantly affect the outcome of any analysis conducted on the database, thus making it possible to prevent private information from exposing to attackers. It follows a rigorous mathematical deduction to prove it can reduce the risk of privacy breach while remaining the utility of the data. At the beginning of this section, we will illuminate the concept by leveraging a simple example. Then, we will give the mathematical definition of differential privay and introduce three privacy mechanisms to achieve it.

\subsection{A Simple Example}
Suppose you have access to a database that allows you to compute the total income of all resident in a certain area. You know one of your friends, Mr. White is going to move to another area, so simply computing the total income of all resident before and after Mr. White's move would allow you to guess his real income. As shown in table \ref{table:1}, the total income of all residents before Mr.White's move is 50 million, while the total income of all residents after Mr.White's move is 49 million. One can compute that the real income of Mr.White is 1 millon. So from this example we can see even though we are not allowed to retrieve the information of a particular person, we are still able to get the private informtion through certain opertions. So what could one do to stop this? In the next section, we wil see how differential privacy resolve this problem.


\begin{table}
	\begin{tabular}{||c||c||}
		\hline
		Name & Annual Income  \\ [0.5ex]
		\hline\hline
		Mr. Richard & 0.5 million  \\
		\hline
		Mr. White & 1 million \\
		\hline
		Mr. Brown & 2 million \\
		\hline
		Ms. Lee & 0.35 million \\
		\hline
		Ms. Jean & 0.6 million \\
		\hline
		... & ...  \\
		\hline
		\multicolumn{2}{||c||}{Total income = 50 million}\\
		\hline
	\end{tabular}
\quad\quad
\begin{tabular}{||c || c||}
	\hline
	Name & Annual Income  \\ [0.5ex]
	\hline\hline
	Mr. Richard & 0.5 million  \\
	\hline
      &  \\
	\hline
	Mr. Brown & 2 million \\
	\hline
	Ms. Lee & 0.35 million \\
	\hline
	Ms. Jean & 0.6 million \\
	\hline
	... & ...  \\
	\hline
	\multicolumn{2}{||c||}{Total income = 49 million}\\
	\hline
\end{tabular}
	\caption{The table before and after Mr. White's move. }
	\label{table:1}
\end{table}

\subsection{Definition of Differential Privacy}
Firstly, let us define some notations.
\theoremstyle{definition}
\begin{definition}{}
 $D$ and $D^\prime$ are databases, but they must differ on at most one row.
\end{definition}
The reason why $D$ and $D^\prime$ is required to differ on one row is to simulate whether a particular record is in or not in the database.
\begin{definition}{}
$f(D)$ is a query on D
\end{definition}
Refer to the previous example, $f(D)$ is the total income of all residents in the database.
\begin{definition}{}
	$M(D)$ is the privacy mechanism, which is a randomized function that takes the database $D$ as inpiut, and release privatized information with respect to $f(D)$.
\end{definition}
Designing privacy mechanism is a topic on its own\cite{mcsherry2007mechanism}. In this survery, we only consider Laplace Mechanism, Exponential Mechanism, and BLR Mechanism. Refer to the previous example, $M(D)$ is the privated total income obtained by adding random noise on the total income.
\begin{definition}{$\epsilon$ - differential privacy}
	A privacy mechanism $M$ gives $\epsilon$ - differential privacy if for all data sets $D$ and $D^\prime$ differing on at most one row, and all $C\in Range(M)$,
	\[  \frac{Pr[M(D) = C]}{Pr[M(D^\prime) = C]}< e^\epsilon \]
\end{definition}
$\epsilon - differential \ privacy$ is a special case of $(\epsilon,\delta) - differential \ privacy$\cite{dwork2011differential,dwork2006our} with $\delta = 0$. Typically, $(\epsilon,\delta) - differential \ privacy$ is simplified to $\epsilon - differential \ privacy$, so we only consider $\epsilon - differential \ privacy$ in this survey. $\epsilon - differential \ privacy$ says that the probability that the privatized result will be $C$ is nearly the same whether or not you are in the database, which means an adversary cannot infer with high confidence (controlled by $\epsilon$) whether the input database is $D$ or $D^\prime$. In the definition, $\epsilon$ is the privacy budget, which is a tradeoff that is used to balance the privacy of the result and it's utility. The smaller the $\epsilon$ is, the closer $Pr[M(D) = C]$ and $Pr[M(D^\prime) = C]$ are, and the stronger protection is.

\subsection{Laplace Mechanism}
In this section, we will introduction one of the most popular privacy mechanisms - Laplace Mechanism\cite{dwork2006calibrating}. Laplace mechanism works particularly well when the query $f(D)\in R^n$ is a funtion mapping databases to (vectors of) real numbers. For example, when the query is a counting query, $f(D)$ is the number of records in the database.
\begin{definition}{Sensitivity of a Function}
For $f: D\rightarrow R^n$, the sensitivity of $f$ is
	\[  \bigtriangleup f = max_{D, D\prime} ||f(D) - f(D\prime)||_n \]
	for all $D, D\prime$ differ on at most one row.
\end{definition}
Intutively, $\bigtriangleup f$ captures how much one person's data can affect the ouput. Taking the counting query as an example, $\bigtriangleup f = 1$ because adding or deleting a row in the database will only affect the number of records by 1.

For simplicity, we only consider the case when $n=1$. Laplace Mechanism $M(D)$ privatizes the result by adding a 0-centered symmetric random noise, which is drawn from Laplace Distribution\cite{simon1774memoire} with parameter $\bigtriangleup f / \epsilon$,  on the true answer $f(D)$. So the probability density function of the random noise $x$ is
   \[ Pr[x] = \frac{\epsilon}{2\bigtriangleup f}e^{-\frac{|x|\epsilon}{\bigtriangleup f}}  \]
The probability density function of $M(D)$ is
  \[ Pr[M(D)] = Pr[x+f(D)] = \frac{\epsilon}{2\bigtriangleup f}e^{-\frac{|x-f(D)|\epsilon}{\bigtriangleup f}}  \]
  The mathematical proof that laplace mechnism yields a $\epsilon - differential \ privacy$ is straightforward.
  \[ \frac{Pr[M(D) = C]}{Pr[M(D\prime = C)]} = \frac{\frac{\epsilon}{2\bigtriangleup f}e^{-\frac{|x-f(D)|\epsilon}{\bigtriangleup f}} }{\frac{\epsilon}{2\bigtriangleup f}e^{-\frac{|x-f(D\prime)|\epsilon}{\bigtriangleup f}} } = \frac{e^{-\frac{|x-f(D)|\epsilon}{\bigtriangleup f}} }{e^{-\frac{|x-f(D\prime)|\epsilon}{\bigtriangleup f}} } \] \[= e^{-\frac{|x-f(D)|\epsilon}{\bigtriangleup f} + \frac{|x-f(D\prime)|\epsilon}{\bigtriangleup f} } = e^{\frac{|f(D) - f(D\prime)|\epsilon}{\bigtriangleup f}} \leq e^\epsilon \]

So it concludes that laplace mechanism yields $\epsilon - differential \ privacy$

\subsection{Exponential Mechanism}
Laplace mechanism is applied to query responses which are appropriately measured on the same scale or in the same units and to which certain magnitude of noise of this scale or units is added. On the contrary, exponential mechanism is first proposed by McSherry \etal \cite{mcsherry2007mechanism} for the situations  in which we wish to choose the "best" response. Following the old scheme by adding noise directly to the computed quantity would completely destroy its accuracy. A simple example took by \cite{dwork2014algorithmic} explains the failure of simply adding noise:

\begin{example}
Suppose in a supermarket a type of chocolate is on sale. The seller has collected a list of bidders: $a$, $b$, $c$, $d$, where $a$, $b$, $c$ each bid $\$1.0$ and $d$ bids $\$3.1$. He wonders how to set the price of the chocolate to maximize the revenue. At $\$3.1$, the revenue is $\$3.1$, at $\$3.0$ and $\$1.0$ the revenue becomes $\$3.0$, but at $\$3.2$ it turns into $\$0.0$.
\end{example}

The exponential mechanism offers a safe solution to answering queries with arbitrary utilities. Given a query with arbitrary range $\mathcal{R}$, exponential mechanism is defined by range $\mathcal{R}$, the privacy parameter $\epsilon$ and a quality function $q:\mathcal{X}^n \times \mathcal{R} \rightarrow \mathbb{R}$, which maps outputs to quality scores. Getting back to the chocolate example, the quality with respect to the price $r\in \mathcal{R}$ and database $D \in  \mathcal{X}^n $ is just the revenue obtained when the price is set to $r$. For a fixed database $D$, the user desires an output that is associated with the maximum quality score. The sensitivity of the quality function, which is a key factor in exponential mechanism, is determined by the database $D$ and the query range $\mathcal{R}$:

\begin{equation}
\triangle = \max_{r \in \mathcal{R}} \max_{D,D':||D-D'||_1 \leq 1} |q(D, r)-q(D',r)|.
\end{equation}

\begin{definition}
Given a database $D \in  \mathcal{X}^n$ and a quality function $q$ with respect to $D$ and query range $\mathcal{R}$ , the exponential mechanism $M_E(D, q, \mathcal{R})$ gives the output $r \in \mathcal{R}$ based on the probability:
$$Pr[M_E(D, q, \mathcal{R}) = r] \propto exp(\frac{\epsilon q(D, r)}{2\triangle}).$$
\end{definition}

\begin{theorem}
The exponential mechanism preserves $(\epsilon, 0)-$differential privacy.
\textit{Proof. Given the query range $\mathcal{R}$, the quality function $q$ and two databases $D, D'$ differing in at most one record (\ie $||D-D'||_1 \leq 1$), the ratio of probabilities that exponential mechanism produces the same output on two databases is }

\begin{align}
\frac{Pr(M_E(D, q, \mathcal{R})=r)}{Pr(M_E(D', q, \mathcal{R})=r)} &=
 \frac{
\left(
\frac{exp( \frac{\epsilon q(D,r) }{2 \triangle})}{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon q(D,r') }{2 \triangle})}
\right)
}
{
\left(
\frac{exp( \frac{\epsilon q(D',r) }{2 \triangle})}{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon q(D',r') }{2 \triangle})}
\right)
} \\
&=
\left(
\frac{exp( \frac{\epsilon q(D,r) }{2 \triangle})}
{exp( \frac{\epsilon q(D',r) }{2 \triangle})}
\right)
\cdot
\left(
\frac{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon q(D',r') }{2 \triangle})}{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon q(D,r') }{2 \triangle})}
\right) \\
& \leq exp \left(
\frac{\epsilon (q(D, r') - q(D', r') )}{2 \triangle}
\right) \\
& \cdot
\left(
\frac{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon (q(D,r')+\triangle) }{2 \triangle})}
{\sum_{r' \in \mathcal{R}} exp( \frac{\epsilon q(D,r') }{2 \triangle})}
\right) \\
& \leq exp(\frac{\epsilon}{2}) \cdot exp(\frac{\epsilon}{2})
 \\
&= exp(\epsilon)
\end{align}
\end{theorem}


The reason why the exponential mechanism can offer strong quality guarantees is that it discounts the probability of outcomes exponentially fast as their quality scores drop. Let $OPT_q(D) = \max_{r \in \mathcal{R}} q(D, r)$ denote the maximum quality score in scope $\mathcal{R}$ with regard to database $D$. The exponential mechanism substantially biases the distribution towards high scoring outputs and brings the expected score close to the optimum $OPT_q(D)$.

\begin{theorem}
A database $D$ and a quality function $q$ with respect to $x$ and query range $\mathcal{R}$ are given. Let $\mathcal{R}_{OPT} =\{r \in \mathcal{R}: q(D, r) = OPT_q(D) \}$ denote the set of elements in $\mathcal{R}$ that assume the maximum score. Then:
\begin{equation}
Pr \left[q(M_E(D, q, \mathcal{R}))
\leq OPT_q(D) - \frac{2\triangle}{\epsilon}
\left(
ln \left( \frac{|\mathcal{R}|}{|\mathcal{R}_{OPT}|} \right) + t
\right) \right]
 \leq e^{-t}
\end{equation}
\textit{Proof. }
\begin{align}  \label{eq:proof2}
Pr[q(M_E(D, q, \mathcal{R})) \leq c]
& \leq \frac{
|\mathcal{R}| exp(\epsilon c / 2 \triangle)
}{
|\mathcal{R}_{OPT}| exp(\epsilon OPT_q(D) / 2 \triangle)
} \\
&= \frac{|\mathcal{R}| }{ | \mathcal{R}_{OPT} | }
exp(
\frac{\epsilon ( c - OPT_q(D)  ) }{2 \triangle}
)
\end{align}
Substitute $c = OPT_q(D) - \frac{2\triangle}{\epsilon}
\left(
ln \left( \frac{|\mathcal{R}|}{|\mathcal{R}_{OPT}|} \right) + t
\right)
$ into above inequation \ref{eq:proof2}, and then Q.E.D.
\end{theorem}
Since we always have $|\mathcal{R}_{OPT}| \geq 1$, the above theorem can be simplified into the following corollary:
\begin{corollary}
Given a database $D$ and a quality function $q$ with respect to the $D$ and query range $\mathcal{R}$, we have:
\begin{equation}
Pr[q(M_E(D, q, \mathcal{R}))  \geq
OPT_q(D) - \frac{2 \triangle}{\epsilon} (ln(| \mathcal{R}|) + t)]
\geq 1-e^{-t}.
\end{equation}
\end{corollary}
In other words, the exponential mechanism is a differential private mechanism that outputs an element from the range that has quality score which is nearly as high as possible---excepting an additive term which is linear in the sensitivity of the quality score and logarithmic in the cardinality of the query range.


\subsection{BLR Mechanism}
BLR mechanism, proposed by Blum, Ligett and Roth \cite{blum2013learning}, is an novel mechanism that breaks the limitation of the number of queries to non-interactive databases. Let $D = \{x_1, x_2, ..., x_n\} \in \mathcal{X}^n$ denote an n-row database and $Q=\{f_1, f_2, ..., f_k\}$ denote a set of queries and frequently write $k=|Q|$. The BLR mechanism preserves accuracy and privacy even when $k \gg n$. The lacpace mechanism requires that each query should be accurate and private independently and independent noise is added to each dimension of the output vector. This strategy reveals information increasingly in linearity to the number of queries $k$. To overcome the rising loss caused by excessive queries, BLR resorts to correlate the noise by projecting the answers to queries in a space of lower dimension. The "projection" is achieved by generating a \textit{synthetic database} that approximately preserves the answers to all queries. Let $\mathcal{X}$ be a universe of data items and $\mathcal{C}$ be a "concept" class consisting of efficiently computable functions $c: \mathcal{X} \rightarrow \{ 0, 1\}$. As analyzed by Blum \etal \cite{blum2013learning}, the synthetic database has the good property of maintaining approximately correct fractional counts for all concepts in $\mathcal{C}$. In other words, for every concept $c \in \mathcal{C}$, the fraction of elements in synthetic database $\hat{D} \in \mathcal{X}^m$ that satisfy $c$ is approximately the same as the fraction of elements in original database $D \in \mathcal{X}^n$ that satisfy $c$. It substantially ensures the accuracy of outputs to queries. And after separating the original database from queries, anyone can run any statistic on it as many times as possible.

We start with the observation that for every database $D$ and query set $Q$, there exists a synthetic database with a small number of rows that preserves the answers to every query in Q.

\begin{theorem}
\textit{
For every $D \in \mathcal{X}^n$ and every set of counting queries $Q$, there exists a synthetic database $\hat{D} \in \mathcal{X}^m$, for $m = \frac{8logk}{\alpha^2}$, such that
$$ max_{i \in [k]} | q_i(D) - q_i(\hat{D}) | \leq \alpha. $$
Proof. Consider a database $\hat{D}\in \mathcal{X}^m$ formed by taking $m < n$ random samples from $D \in \mathcal{X}^n$. Then by a union bound and a Chernoff bound
\begin{align}
Pr\left[ max_{q_i \in Q} | q_i(D) - q_i(\hat{D})   | > \alpha \right]
&\leq  k \cdot Pr \left[ | q_1(D) - q_1(\hat{D})   |  > \alpha  \right] \\
& \leq k \cdot exp\left( \frac{-\alpha^2 m}{4} \right) \\
&= k \cdot exp(-2logk) \\
& < 1
\end{align}
}
It suggests that there must exists some $\hat{D} \in \mathcal{X}^m$ that preserves the answers to every query $q \in Q$ up to an additive error term of $\alpha$.
\end{theorem}

The above theorem has kindled interest in synthetic databases \cite{blum2013learning, dwork2009complexity}. But how to construct the differentially private synthetic databases efficiently? The BLR mechanism \cite{blum2013learning} presents a classical paradigm that instantiates the exponential mechanism to build up the synthetic database:
\begin{itemize}
\item Let $\mathcal{R} = \mathcal{X}^m$ for $m = \frac{32logk}{\alpha^2}$.
\item Let the quality function $q(D, \hat{D}) = -max_{f\in Q} | f(D) - f(\hat{D}) |$ for every $D \in \mathcal{X}^n$ and $\hat{D} \in \mathcal{X}^m$. Note that we make the quality function inversely related to the error so that better accuracy implies higher score.
\item The sensitivity of the quality function $\triangle = 1/n$, because it is simply the maximum error over a set of counting queries, and a counting query  has global sensitivity $1/n$.
\item Sample and output $\hat{D} \in \mathcal{X}^m$ with the exponential mechanism $M_E(D, q, \mathcal{R})$.
\end{itemize}
However, the BLR mechanism is inefficient. In general, the running time will be at least polynomial in $|\mathcal{X}|$. So it is a significant open question to understand whether or not we can achieve accuracy and differential privacy in time polylog $|\mathcal{X}|$ for various classes of queries.






\section{Local Differential Privacy}
In traditional differential privacy\cite{dwork2008differential}, all sensitive information from a large number of respondents are gathered by a trusted and trustworthy curator, who further releases the statistical information of the underlying population to the public. The reponsibiliy to privatize information lies on the curator side. However, in Local Differential Privacy (LDP)\cite{gupta2013privately,kasiviswanathan2011can}, every respondent take the responsibility to privatize his or her data locally before sending to the curator. So in this setting, the curator will never have the access to the exact value of sensitive information, which protects not only the privacy of data contributors but also the curator itself against the potential risk of information leakage. The goal of LDP is two fold: (1) $\epsilon - differential \ privacy$ must be satisfied on each user side. (2) the curator should be able to compute accurate statistics from the noisy data received from the user side. It turns out that traditional differential privacy mechanisms are not adequate enough to address the LDP problems. In the following, we overview several typical LDP solutions.
\subsection{Randomized Response}
Rendomized Response (RR)\cite{erlingsson2014rappor} asks each user with a sensitive question whose answer must be "yes" or "no". For example, "do you like Donald Trump?". The user flips a coin to decide which answer to give. The user gives his true answer when the coin turns head, and gives the opposite answer when the coin turns tail. However, in RR, instead of using a fair coin, we use a biased coin. It turns head with probability $p$, and turns tail with probability $1-p$. It turns out that $\epsilon - differential \ privacy$ can be satisfied with the following value of p:
\[ p = \frac{e^\epsilon}{1+e^\epsilon} \]
The goal of the curator is to give the estimated percentage of "yes" given the noisy answers. Suppose the percentage of "yes" given the noisy answer is $c$, the corrected version of the result $c^\prime$ is:
\[ c^\prime = c \times c_{e}, where \ c_{e} = \frac{1}{1-2p}\]
The limitation of RR is that it can only be applied to the problem with binary answer.
\subsection{RAPPOR}
RAPPOR\cite{erlingsson2014rappor,fanti2016building} extends RR\cite{erlingsson2014rappor} to more complex data types. Suppose there are $n$ users and $d$ items, each user can own exactly one item. To be more specific, let us define $u_i$, where $i=1 \ to \ n$, as the $i_{th}$ user. $v_i$, where $i=1 \ to \ n$, is a vector with length $d$ of $u_i$. All the coordinates of $v_i$ are 0 except the $j_{th}$ coordinate, which is 1, if $u_i$ owns item $j$. The goal of the curator is the same as in RR, which is to compute the frequency of each item. User $u_i$ applies RR independently on each coordinate of $v_i$ with a biased coin with probability $p$ (the sensitivity of any query function $f$ is 2 because any vector contains a single coordinate of 1, hence the maximum difference between $f(D)$ and $f(D^\prime)$ is 2):
\[ p = \frac{e^{\frac{\epsilon}{2}}}{1+e^{\frac{\epsilon}{2}}} \]
The limitation of RAPPOR is that it can cause huge communication overhead because each user has to send a vector with lengh $d$, which can be extremly large in some cases (e.g. the number of email accounts of all users).

\subsection{Succinct histogram}
Succinct histogram (SH)\cite{bassily2015local} addresses the communication overhead led by RAPPOR\cite{erlingsson2014rappor,fanti2016building}. Basically, the problem setting of SH remains the same with RAPPOR, i.e. each user $u_i$ owns a vector $v_i$ of length $d$, with only one coordinate to be 1 and all the others to be 0. The goal of the curator is also to estimate the frequency of each item. The main idea behind SH is instead of sending $d$ coordinates every time, every user only randomly pick one coordinate and report it to the curator. In this way, the communication overhead drops from $O(d)$ to $O(1)$.
\subsection{LDPMiner}
The above mentioned solutions pose too much limitations on the data types. In order to cope with complex data types, Zhan et al. proposed LDPMiner\cite{qin2016heavy} to address the heavy hitters over set-valued data. Suppose there are $n$ users and $d$ items, and each user can own exactly $l$ items. If the number of the items a user owns is smaller than $l$, some dummy items are added to fill the set. If the number of the items a user owns is greater than $l$, $l$ randomly picked items are considered in the set. The frequency of the item is the portion of users owning this item. The general goal of the curator is to find the top-k most frequent items with the highest frequency. LDPMiner proposes a two-phase solution. The main idea is to firstly filter the items and select $O(k)$ candidate heavy hitters in the first phase, and then focuses on refining the frequecy estimates of these candidates in the second phase. LDPMiner splits the total privay budget $\epsilon$ as $\epsilon_1$ and $\epsilon_2$, which is assigned to phase one and phase two respectively. According to sequential composability\cite{mcsherry2009privacy}, the whole process satisfies $\epsilon - differential \ privacy$.
%------------------------------------------------

\section*{Conclusion}
We have surveyed a number of approaches in the context of different privacy which is triggered by the rising aspiration for data publishment and privacy protection. Targeted at various requirements and situations, a large literature has explored abundant differentially private mechanisms \cite{mcsherry2007mechanism}, three of which has been introduced in this writing. A number of sub-fields of differential privacy like local differential privacy have also been investigated to apply it into more specific applications. Although we may not notice, differential privacy is supporting this world full of information and knowledge in its way.




%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}

\bibliography{sample}

%----------------------------------------------------------------------------------------

\end{document}
